{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click 'Run Cell' button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/ipython-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "# video reference https://www.youtube.com/watch?v=pcsHj09ioMg"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## capture video normal ",
   "id": "26ff777ca2bd64ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "mpFaceMesh = mp.solutions.face_mesh\n",
    "\n",
    "with mpFaceMesh.FaceMesh(\n",
    "        static_image_mode=True,\n",
    "        max_num_faces=1) as face_mesh:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame=cv2.flip(frame, 1)\n",
    "        \n",
    "        height, width, _ = frame.shape\n",
    "        frameRgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(frameRgb)\n",
    "\n",
    "        \n",
    "        cv2.imshow('frame', frame)\n",
    "        key=cv2.waitKey(20) & 0xFF\n",
    "        if key==27:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "34397ea5d6f2cb4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## capture video normal",
   "id": "a8635fbe40f5524a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "mpFaceMesh = mp.solutions.face_mesh\n",
    "\n",
    "indexLeftEye = [33, 160, 158, 133, 153, 144]\n",
    "indexRightEye = [362, 385, 387, 263, 373, 380]\n",
    "\n",
    "with mpFaceMesh.FaceMesh(\n",
    "        static_image_mode=True,\n",
    "        max_num_faces=1) as face_mesh:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame=cv2.flip(frame, 1)\n",
    "\n",
    "        height, width, _ = frame.shape\n",
    "        frameRgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(frameRgb)\n",
    "\n",
    "        coordinates_left_eye = []\n",
    "        coordinates_right_eye = []\n",
    "\n",
    "        if results.multi_face_landmarks is not None:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                for index in indexLeftEye:\n",
    "                    x = int(face_landmarks.landmark[index].x * width)\n",
    "                    y = int(face_landmarks.landmark[index].y * height)\n",
    "                    coordinates_left_eye.append([x, y])\n",
    "                    cv2.circle(frame, (x, y), 2, (0, 255, 255), 1)\n",
    "                    cv2.circle(frame, (x, y), 1, (128, 0, 250), 1)\n",
    "                for index in indexRightEye:\n",
    "                    x = int(face_landmarks.landmark[index].x * width)\n",
    "                    y = int(face_landmarks.landmark[index].y * height)\n",
    "                    coordinates_right_eye.append([x, y])\n",
    "                    cv2.circle(frame, (x, y), 2, (128, 0, 250), 1)\n",
    "                    cv2.circle(frame, (x, y), 1, (0, 255, 255), 1)\n",
    "\n",
    "        cv2.imshow('frame', frame)\n",
    "        key=cv2.waitKey(20) & 0xFF\n",
    "        if key==27:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "9a5a93ae32bc4b05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## FINISH DETECTOR",
   "id": "da1b3fc77298fbbc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "mpFaceMesh = mp.solutions.face_mesh\n",
    "\n",
    "EAR_THRESH=0.24\n",
    "count=0\n",
    "indexLeftEye = [33, 160, 158, 133, 153, 144]\n",
    "indexRightEye = [362, 385, 387, 263, 373, 380]\n",
    "\n",
    "def eye_aspect_ratio(coordinates):\n",
    "    d_A = np.linalg.norm(np.array(coordinates[1]) - np.array(coordinates[5]))\n",
    "    d_B = np.linalg.norm(np.array(coordinates[2]) - np.array(coordinates[4]))\n",
    "    d_C = np.linalg.norm(np.array(coordinates[0]) - np.array(coordinates[3]))\n",
    "    return (d_A + d_B) / (2 * d_C)\n",
    "\n",
    "\n",
    "with mpFaceMesh.FaceMesh(\n",
    "        static_image_mode=True,\n",
    "        max_num_faces=1) as face_mesh:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame=cv2.flip(frame, 1)\n",
    "\n",
    "        height, width, _ = frame.shape\n",
    "        frameRgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(frameRgb)\n",
    "\n",
    "        coordinates_left_eye = []\n",
    "        coordinates_right_eye = []\n",
    "\n",
    "        if results.multi_face_landmarks is not None:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                for index in indexLeftEye:\n",
    "                    x = int(face_landmarks.landmark[index].x * width)\n",
    "                    y = int(face_landmarks.landmark[index].y * height)\n",
    "                    coordinates_left_eye.append([x, y])\n",
    "                    cv2.circle(frame, (x, y), 2, (0, 255, 255), 1)\n",
    "                    cv2.circle(frame, (x, y), 1, (128, 0, 250), 1)\n",
    "                for index in indexRightEye:\n",
    "                    x = int(face_landmarks.landmark[index].x * width)\n",
    "                    y = int(face_landmarks.landmark[index].y * height)\n",
    "                    coordinates_right_eye.append([x, y])\n",
    "                    cv2.circle(frame, (x, y), 2, (128, 0, 250), 1)\n",
    "                    cv2.circle(frame, (x, y), 1, (0, 255, 255), 1)\n",
    "\n",
    "            ear_left_eye = eye_aspect_ratio(coordinates_left_eye)\n",
    "            ear_right_eye = eye_aspect_ratio(coordinates_right_eye)\n",
    "            ear = (ear_left_eye + ear_right_eye)/2\n",
    "            \n",
    "            if ear > EAR_THRESH and count <= 30:\n",
    "                count += 1\n",
    "            elif ear < EAR_THRESH and count >= -30:\n",
    "                count -=1\n",
    "            if(count>0):\n",
    "                print(\"DESPIERTO\"+str(count))\n",
    "            else:\n",
    "                print(\"DORMIDO\"+str(count))\n",
    "        \n",
    "        cv2.imshow('frame', frame)\n",
    "        key=cv2.waitKey(20) & 0xFF\n",
    "        if key==27:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "id": "d682f7b0b821a59a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CODE REFERENCE EYELID DETECTOR\n",
   "id": "33a92c84451da55b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T20:45:19.161077Z",
     "start_time": "2024-10-18T20:45:14.792150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def drawing_output(frame, coordinates_left_eye, coordinates_right_eye, blink_counter):\n",
    "    aux_image = np.zeros(frame.shape, np.uint8)\n",
    "    contours1 = np.array([coordinates_left_eye])\n",
    "    contours2 = np.array([coordinates_right_eye])\n",
    "    cv2.fillPoly(aux_image, pts=[contours1], color=(255, 0, 0))\n",
    "    cv2.fillPoly(aux_image, pts=[contours2], color=(255, 0, 0))\n",
    "    output = cv2.addWeighted(frame, 1, aux_image, 0.7, 1)\n",
    "    cv2.rectangle(output, (0, 0), (200, 50), (255, 0, 0), -1)\n",
    "    cv2.rectangle(output, (202, 0), (265, 50), (255, 0, 0), 2)\n",
    "    cv2.putText(output, \"Num. Parpadeos:\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "    cv2.putText(output, \"{}\".format(blink_counter), (220, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (128, 0, 250), 2)\n",
    "\n",
    "    return output\n",
    "\n",
    "def eye_aspect_ratio(coordinates):\n",
    "    d_A = np.linalg.norm(np.array(coordinates[1]) - np.array(coordinates[5]))\n",
    "    d_B = np.linalg.norm(np.array(coordinates[2]) - np.array(coordinates[4]))\n",
    "    d_C = np.linalg.norm(np.array(coordinates[0]) - np.array(coordinates[3]))\n",
    "    return (d_A + d_B) / (2.0 * d_C)\n",
    "\n",
    "def plotting_ear(pts_ear, line1):\n",
    "    global figure\n",
    "    pts = np.linspace(0, 1, len(pts_ear))\n",
    "    if line1 == []:\n",
    "        plt.style.use(\"ggplot\")\n",
    "        plt.ion()\n",
    "        figure, ax = plt.subplots()\n",
    "        line1, = ax.plot(pts, list(pts_ear))\n",
    "        plt.ylim(0.1, 0.4)\n",
    "        plt.xlim(0, 1)\n",
    "        plt.ylabel(\"EAR\", fontsize=18)\n",
    "    else:\n",
    "        line1.set_ydata(list(pts_ear))\n",
    "        figure.canvas.draw()\n",
    "        figure.canvas.flush_events()\n",
    "\n",
    "    return line1\n",
    "\n",
    "# Configuración inicial\n",
    "cap = cv2.VideoCapture(0)\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "index_left_eye = [33, 160, 158, 133, 153, 144]\n",
    "index_right_eye = [362, 385, 387, 263, 373, 380]\n",
    "EAR_THRESH = 0.26\n",
    "NUM_FRAMES = 2\n",
    "aux_counter = 0\n",
    "blink_counter = 0\n",
    "line1 = []\n",
    "pts_ear = deque(maxlen=64)\n",
    "i = 0\n",
    "\n",
    "with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=False,\n",
    "        max_num_faces=1,\n",
    "        refine_landmarks=True) as face_mesh:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        height, width, _ = frame.shape\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(frame_rgb)\n",
    "        coordinates_left_eye = []\n",
    "        coordinates_right_eye = []\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                for index in index_left_eye:\n",
    "                    x = int(face_landmarks.landmark[index].x * width)\n",
    "                    y = int(face_landmarks.landmark[index].y * height)\n",
    "                    coordinates_left_eye.append([x, y])\n",
    "                    cv2.circle(frame, (x, y), 2, (0, 255, 255), 1)\n",
    "                for index in index_right_eye:\n",
    "                    x = int(face_landmarks.landmark[index].x * width)\n",
    "                    y = int(face_landmarks.landmark[index].y * height)\n",
    "                    coordinates_right_eye.append([x, y])\n",
    "                    cv2.circle(frame, (x, y), 2, (128, 0, 250), 1)\n",
    "\n",
    "            # Cálculo de la relación de aspecto del ojo\n",
    "            ear_left_eye = eye_aspect_ratio(coordinates_left_eye)\n",
    "            ear_right_eye = eye_aspect_ratio(coordinates_right_eye)\n",
    "            ear = (ear_left_eye + ear_right_eye) / 2.0\n",
    "\n",
    "            # Detección de parpadeo\n",
    "            if ear < EAR_THRESH:\n",
    "                aux_counter += 1\n",
    "            else:\n",
    "                if aux_counter >= NUM_FRAMES:\n",
    "                    aux_counter = 0\n",
    "                    blink_counter += 1\n",
    "\n",
    "            # Dibujar resultados\n",
    "            frame = drawing_output(frame, coordinates_left_eye, coordinates_right_eye, blink_counter)\n",
    "            pts_ear.append(ear)\n",
    "            if i > 70:\n",
    "                line1 = plotting_ear(pts_ear, line1)\n",
    "            i += 1\n",
    "\n",
    "        # Mostrar el frame\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "        if k == 27:  # ESC para salir\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "id": "56898d3479b519f6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libEGL warning: MESA-LOADER: failed to open iris: /usr/lib/dri/iris_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "libEGL warning: MESA-LOADER: failed to open zink: /usr/lib/dri/zink_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "libEGL warning: MESA-LOADER: failed to open iris: /usr/lib/dri/iris_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "libEGL warning: MESA-LOADER: failed to open zink: /usr/lib/dri/zink_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "libEGL warning: MESA-LOADER: failed to open iris: /usr/lib/dri/iris_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "libEGL warning: MESA-LOADER: failed to open zink: /usr/lib/dri/zink_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "W0000 00:00:1729284315.005146   44357 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729284315.023749   44358 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8a514a8d7ad8a0cb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
